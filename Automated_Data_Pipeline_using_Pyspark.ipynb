{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14FNPlDN94rOWfvp248IK3udxWDEt_N5C",
      "authorship_tag": "ABX9TyNE6fu1bF/V/lOU5l3Unulo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siva-Parvathi-M/Siva/blob/main/Automated_Data_Pipeline_using_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up-V9RvEaDZT",
        "outputId": "adfe06be-0ba0-4913-c6a2-e2880c7f3e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "YXD0yE4IaRcY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GoogleDrive_Pipeline\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "JIS83OOLbsdi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read Data from Google Drive**"
      ],
      "metadata": {
        "id": "E4VCl0_bkYW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/drive/MyDrive/Colab Notebooks/Input Files/state-borrowings-year-wise-total-amount-of-market-borrowings-by-state-governments-since-2006-07.csv\"\n",
        "df = spark.read.option(\"header\", \"true\").csv(input_path)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0whykvuwb26I",
        "outputId": "790960c8-ec1b-435e-fa27-8ee450184e4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "|fiscal_year|gross_allocation_in_crores|repayments_in_crores|net_allocation_in_crores|\n",
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "|    2022-23|                    758392|              239561|                  518829|\n",
            "|    2021-22|                    701626|              209143|                  492483|\n",
            "|    2020-21|                    798816|              147039|                  651777|\n",
            "|    2019-20|                    634521|              147067|                  487454|\n",
            "|    2018-19|                    478324|              129680|                  348643|\n",
            "|    2017-18|                    419100|               78819|                  340281|\n",
            "|    2016-17|                    381979|               39290|                  342689|\n",
            "|    2015-16|                    294560|               33370|                  261190|\n",
            "|    2014-15|                    269840|               33380|                  236460|\n",
            "|    2013-14|                    250610|               32080|                  218530|\n",
            "|    2012-13|                    218710|               30630|                  188080|\n",
            "|    2011-12|                    167860|               21990|                  145870|\n",
            "|    2010-11|                    157200|               15640|                  142160|\n",
            "|    2009-10|                    118190|               16240|                  104940|\n",
            "|    2008-09|                    129080|               14370|                  114710|\n",
            "|    2007-08|                     80570|               11560|                   69020|\n",
            "|    2006-07|                     26600|                6550|                   20050|\n",
            "|    2005-06|                     21730|                6270|                   15450|\n",
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform Data**"
      ],
      "metadata": {
        "id": "q2six5m3knpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(df[\"repayments_in_crores\"] > 50000)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsolz_eOc0rP",
        "outputId": "f95628dc-9a30-487f-fb0d-f1e4f4b999bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "|fiscal_year|gross_allocation_in_crores|repayments_in_crores|net_allocation_in_crores|\n",
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "|    2022-23|                    758392|              239561|                  518829|\n",
            "|    2021-22|                    701626|              209143|                  492483|\n",
            "|    2020-21|                    798816|              147039|                  651777|\n",
            "|    2019-20|                    634521|              147067|                  487454|\n",
            "|    2018-19|                    478324|              129680|                  348643|\n",
            "|    2017-18|                    419100|               78819|                  340281|\n",
            "+-----------+--------------------------+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Processed Data to Google Drive**"
      ],
      "metadata": {
        "id": "RnNTmHcmkxJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/Target Files\"\n",
        "\n",
        "df_filtered.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)"
      ],
      "metadata": {
        "id": "8ZA_SRTsdvOT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automate the Pipeline**"
      ],
      "metadata": {
        "id": "7XwXqyyGk1ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# ‚úÖ Define the Data Pipeline Function\n",
        "def run_pipeline():\n",
        "    print(\"üöÄ Running PySpark Data Pipeline...\")\n",
        "\n",
        "    # Example processing step (Replace this with actual PySpark code)\n",
        "    print(\"‚úÖ Data Processing Completed Successfully!\")\n",
        "\n",
        "# ‚úÖ Run Pipeline Every 30 Seconds\n",
        "while True:\n",
        "    run_pipeline()  # Call function inside the loop\n",
        "    print(\"‚è≥ Waiting for next run... (30 sec)\")\n",
        "    time.sleep(30)  # Wait for 30 sec before running again"
      ],
      "metadata": {
        "id": "-dDZ0rcnjanD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How This Works ‚úÖ Fully automated pipeline runs inside PySpark. ‚úÖ Extracts data from Google Drive. ‚úÖ Transforms (Filtering & Cleaning). ‚úÖ Loads processed data back to Google Drive. ‚úÖ Runs every 6 hours (Change time.sleep() as needed).**"
      ],
      "metadata": {
        "id": "m_36xAvelQXY"
      }
    }
  ]
}